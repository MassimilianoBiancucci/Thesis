\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Un esempio preso dal \textbf {Severstal steel defect dataset} di difetti segmentati. credits: Neven Robby and Goedemé Toon, 2021, A Multi-Branch U-Net for Steel Surface Defect Type and Severity Segmentation. https://www.mdpi.com/2075-4701/11/6/870\relax }}{2}{figure.caption.3}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces Esempio di architettura a solo decoder.\relax }}{4}{figure.caption.5}%
\contentsline {figure}{\numberline {1.3}{\ignorespaces Esempio di architettura con encoder e decoder.\relax }}{5}{figure.caption.7}%
\contentsline {figure}{\numberline {1.4}{\ignorespaces Un diagramma di ven che illustra le relazioni tra i diversi sottogruppi dell'intelligenza artificiale, vediamo infatti come il deep learning sia un sottogruppo del representation learning, che a sua volta è un sottogruppo del machine learning. credits: Yoshua Bengio, Ian J. Goodfellow, Aaron Courville 2015, From the book "Deep Learning" \relax }}{6}{figure.caption.8}%
\contentsline {figure}{\numberline {1.5}{\ignorespaces Schema di un neurone biologico.\relax }}{7}{figure.caption.9}%
\contentsline {figure}{\numberline {1.6}{\ignorespaces Esempio di rete neurale feedforward. In tale rete è possibile vedere i neuroni rappresentati dai nodi del grafo, e le interconnessioni tra di essi che definiscono il peso di ogni relazione, con in rosso un peso positivo e in blu un peso negativo, l'intensità del colore indica la forza della relazione.\relax }}{8}{figure.caption.10}%
\contentsline {figure}{\numberline {1.7}{\ignorespaces Esempio di Neurone artificiale.\relax }}{8}{figure.caption.11}%
\contentsline {figure}{\numberline {1.9}{\ignorespaces Esempio di retropropagazione su di una rete semplificata a 2 strati.\relax }}{12}{figure.caption.13}%
\contentsline {figure}{\numberline {1.10}{\ignorespaces L'immagine raffigura schematicamente il funzionamento di Neocognitron. credits: Kunihiko Fukushima 1980 \cite {Fukushima1980Neocognitron}.\relax }}{14}{figure.caption.14}%
\contentsline {figure}{\numberline {1.11}{\ignorespaces L'immagine tratta dallo stesso articolo illustra schematicamente l'architettura del modello TDNN. credits: Alex Waibel et al. 1989 \cite {Waibel1989PhonemeRecognition}.\relax }}{14}{figure.caption.15}%
\contentsline {figure}{\numberline {1.12}{\ignorespaces L'immagine tratta dal medesimo articolo illustra l'architettura proposta. credits: Yann LeCun et al. 1989 \cite {LeCun1989Backpropagation}.\relax }}{15}{figure.caption.16}%
\contentsline {figure}{\numberline {1.13}{\ignorespaces L'immagine illustra l'operazione di convoluzione. credits: Wikipedia. https://en.wikipedia.org /wiki/Convolution\relax }}{16}{figure.caption.17}%
\contentsline {figure}{\numberline {1.14}{\ignorespaces Operazione di convoluzione 2D discreta, in blu abbiamo una matrice 5x5 che rappresenta un'immagine, mentre in verde il risultato di una convoluzione, con un kernel 3x3. I valori del kernel sono raffigurati in basso a destra delle caselle interessate dalla convoluzione. credits: Dumoulin et al. 2016 \cite {dumoulin2016guide}.\relax }}{16}{figure.caption.18}%
\contentsline {figure}{\numberline {1.15}{\ignorespaces Immagine di Lena Forsén, prima e dopo la convoluzione con i due kernel, che amplificano i bordi verticali e orizzontali.\relax }}{18}{figure.caption.19}%
\contentsline {figure}{\numberline {1.16}{\ignorespaces Esempio di convoluzione con un kernel 3x3, stride pari a 2, e padding (zero padding) pari a 1. credits: Dumoulin et al. 2016 \cite {dumoulin2016guide}\relax }}{19}{figure.caption.23}%
\contentsline {figure}{\numberline {1.17}{\ignorespaces Esempio di max pooling con una finestra di dimensione 3x3, stride pari a 1 e padding 0. credits: Dumoulin et al. 2016 \cite {dumoulin2016guide}\relax }}{21}{figure.caption.27}%
\contentsline {figure}{\numberline {1.18}{\ignorespaces Esempio di average pooling con una finestra di dimensione 3x3, stride pari a 1 e padding 0. credits: Dumoulin et al. 2016 \cite {dumoulin2016guide}\relax }}{21}{figure.caption.29}%
\contentsline {figure}{\numberline {1.19}{\ignorespaces Esempio di convoluzione di un'immagine rgb. credits: Irhum Shafkat \cite {shafkat2018convolutional}\relax }}{22}{figure.caption.30}%
\contentsline {figure}{\numberline {1.20}{\ignorespaces Esempio di convoluzione multichannel. credits: Irhum Shafkat \cite {shafkat2018convolutional}\relax }}{22}{figure.caption.31}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Rappresentazione grafica della training pipeline di un modello GAN.\relax }}{24}{figure.caption.32}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Confronto tra MSE e Adversarial training. credits: Goodfellow \cite {goodfellow2016ganintro}\relax }}{25}{figure.caption.33}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Visualizzazione schematica della convergenza della distribuzione dei dati generati verso quelli reali, e dell'uscita del discriminatore al variare di $\mathbf {x}$. Si noti che la linea nera tratteggiata rappresenta la distribuzione dei dati reali $p_{data}$, mentre la linea verde continua rappresenta la distribuzione dei dati generati $p_{model}$ e in fine la linea blu tratteggiata rappresenta l'uscita del discriminatore $D(x)$, al variare di $\mathbf {x}$. I 2 assi in fondo sono rispettivamente, l'asse $z$ i valori casuali di input del generatore con distribuzione fissa, e l'asse $x$, i valori di input del discriminatore, appartenenti alla distribuzione $\mathbf {p_{data}}$ o $\mathbf {p_{g}}$. credits: Goodfellow et al. \cite {goodfellow2014generative}\relax }}{27}{figure.caption.35}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces Esempio di corretto apprendimento (prima riga) e di mode collapse (seconda riga). credits: Luke Metz et al. \cite {metz2017unrolled}\relax }}{29}{figure.caption.37}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces Alcuni risultati del modello addestrato in questa ricerca, relativi a un dataset di facce umane in bassa risoluzione (b) e il dataset Minst (a). credits: Goodfellow et al. \cite {goodfellow2014generative}\relax }}{29}{figure.caption.38}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces Architettura del generatore di DCGAN. credits: Alec Radford et al. \cite {radford2016unsupervised}\relax }}{30}{figure.caption.39}%
\contentsline {figure}{\numberline {2.9}{\ignorespaces Esempio di algebra vettoriale nello spazio latente Z. credits: Alec Radford et al. \cite {radford2016unsupervised}\relax }}{31}{figure.caption.40}%
\contentsline {figure}{\numberline {2.10}{\ignorespaces Esempio di \textit {gradient vanishing}, causato da un discriminatore addestrato fino all'ottimo per un generatore non ottimo.\relax }}{32}{figure.caption.41}%
\contentsline {figure}{\numberline {2.11}{\ignorespaces In questa immagine è possibile vedere la derivata della funzione seno, nei punti $\pi /4$, $\pi /2$ e $\pi $, ed è possibile vedere come tale funzione rispetta la \textit {1-Lipschitz continuity}, essendo la sua derivata all'interno dell'intervallo ammesso in ogni punto.\relax }}{34}{figure.caption.42}%
\contentsline {figure}{\numberline {2.12}{\ignorespaces In questa immagine è possibile vedere la derivata della funzione $x^2$, nei punti 0.2, 0.4 e 0.8, in questo caso la funzione non rispetta la \textit {1-Lipschitz continuity}, in quanto la sua derivata cresce rapidamente oltre il limite consentito dopo x=0.4.\relax }}{34}{figure.caption.43}%
\contentsline {figure}{\numberline {2.13}{\ignorespaces In questa immagine è possibile vedere come il critico $C(x)$ sia in grado di mappare efficacemente la distanza tra due distribuzioni $P_{data}$ e $P_{G}$, restituendo dei gradienti utili per il generatore anche quando il critico è addestrato all'ottimo per il dato generatore.\relax }}{34}{figure.caption.44}%
\contentsline {figure}{\numberline {2.14}{\ignorespaces Andamento del training di una DCGAN e di una MLP addestrate utilizzando la \textit {Wasserstein distance}. credits: Martin Arjovsky et al. \cite {arjovsky2017wasserstein}\relax }}{35}{figure.caption.45}%
\contentsline {figure}{\numberline {2.15}{\ignorespaces Andamento del training di una DCGAN e di una MLP addestrate utilizzando la \textit {Jensen-Shannon divergence}. credits: Martin Arjovsky et al. \cite {arjovsky2017wasserstein}\relax }}{35}{figure.caption.46}%
\contentsline {figure}{\numberline {2.16}{\ignorespaces In questa immagine è possibile vedere un esempio di training di Pix2Pix per il task \textit {edges to photo}, In tale configurazione il discriminatore apprende come discernere tra coppie di immagini e schizzi, e si può vedere come diversamente dal caso classico sia il generatore che il discriminatore ricevono in ingresso l'immagine di riferimento (lo schizzo) credits: Phillip Isola et al. \cite {isola2018imagetoimage}\relax }}{37}{figure.caption.47}%
\contentsline {figure}{\numberline {2.17}{\ignorespaces In questa immagine è illustrata intuitivamente la differenza tra un comune modello ad \textit {encoder-decoder} e l'architettura di U-net. credits: Phillip Isola et al. \cite {isola2018imagetoimage}\relax }}{37}{figure.caption.48}%
\contentsline {figure}{\numberline {2.18}{\ignorespaces In questa immagine è possibile vedere una comparazione di un sample generato da 4 diversi modelli per il task \textit {labels to scene}, i quattro esempi illustrano l'utilizzo o meno delle \textit {skip connection} e l'utilizzo di una loss L1 a una loss L1 più \textit {conditional adversarial loss}. credits: Phillip Isola et al. \cite {isola2018imagetoimage}\relax }}{38}{figure.caption.49}%
\contentsline {figure}{\numberline {2.19}{\ignorespaces In questa immagine sono mostrati alcuni esempi di applicazione di pix2pix. credits: Phillip Isola et al. \cite {isola2018imagetoimage}\relax }}{38}{figure.caption.50}%
\contentsline {figure}{\numberline {2.20}{\ignorespaces In questa immagine è illustrata schematicamente la struttura di LaMa. credits: Roman Suvorov et al. \cite {suvorov2021resolutionrobust}\relax }}{39}{figure.caption.51}%
\contentsline {figure}{\numberline {2.21}{\ignorespaces In questa immagine sono mostrati schematicamente i passaggi che permettono di calcolare la perceptual loss da un modello convoluzionale. credits: Richard Zhang et al. \cite {zhang2018unreasonable}\relax }}{42}{figure.caption.55}%
\contentsline {figure}{\numberline {2.22}{\ignorespaces In questa immagine sono mostrati i risultati ottenuti da Mescheder et al. in \cite {mescheder2018training} utilizzando la R1 regularization. In blu è rappresentata la distribuzione di riferimento, in arancione i campioni generati, mentre da viola a giallo si ha l'intensità del gradiente di $D$.\relax }}{44}{figure.caption.58}%
\contentsline {figure}{\numberline {2.23}{\ignorespaces In questa immagine sono mostrati alcuni risultati ottenuti da LaMa. In blu sono rappresentate le maschere applicate alle immagini, e dunque corrispondenti alle arre che sono state rimosse prima della propagazione nella rete per la ricostruzione. Le immagini sulla destra rappresentano l'output del modello. credits: Roman Suvorov et al. \cite {suvorov2021resolutionrobust}\relax }}{45}{figure.caption.60}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Figura che illustra un esempio di input di COIGAN, il quale è un tensore di 7 canali, in questo caso con un'immagine RGB e 4 canali per le maschere che effettuano il condizionamento della generazione dei difetti.\relax }}{54}{figure.caption.69}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Figura che illustra in maniera riassuntiva le componenti principali della pipeline di addestramento di COIGAN. le frecce rappresentano la propagazione dei vari tensori, mentre le linee rosse indicano la propagazione dei gradienti delle loss.\relax }}{54}{figure.caption.70}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Funzione che effettua la decodifica di una maschera codificata in RLE.\relax }}{56}{figure.caption.72}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces Esempio di file contenente gli indici degli esempi.\relax }}{57}{figure.caption.73}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces Esempio di file contenente le annotazioni nel formato \textit {line json}.\relax }}{57}{figure.caption.74}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces Figura che illustra il processo di creazione dei dataset degli oggetti. In questo caso viene mostrato come vengono estratti i difetti da una singola immagine contenente 3 difetti, i quali vengono smistati nei rispettivi dataset.\relax }}{59}{figure.caption.77}%
\contentsline {figure}{\numberline {4.8}{\ignorespaces Esempio di file contenente le annotazioni degli object dataset nel formato \textit {line json}.\relax }}{59}{figure.caption.78}%
\contentsline {figure}{\numberline {4.9}{\ignorespaces Figura che illustra concettualmente il processo di tiling delle immagini.\relax }}{60}{figure.caption.79}%
\contentsline {figure}{\numberline {4.10}{\ignorespaces In Figura è mostrato il diagramma UML semplificato del dataloader e delle classi principali che lo compongono. Questo infatti suddivide diverse operazioni di caricamento dei dati in classi separate, le quali fanno convergere i risultati all'oggetto \textbf {CoiganSeverstalSteelDefectsDataset} che restituisce un output compatibile con l'oggetto \textbf {torch.utils.data.DataLoader}.\relax }}{62}{figure.caption.80}%
\contentsline {figure}{\numberline {4.11}{\ignorespaces Figura che illustra concettualmente il processo di caricamento degli oggetti da parte del \textbf {ObjectDataloader}.\relax }}{64}{figure.caption.81}%
\contentsline {figure}{\numberline {4.12}{\ignorespaces Figura che illustra un esempio di applicazione del \textbf {GassianNoiseGenerator}.\relax }}{65}{figure.caption.82}%
\contentsline {figure}{\numberline {4.13}{\ignorespaces Figura che illustra un esempio di generazione di rumore all'interno del \textbf {MultiscaleNoiseGenerator}.\relax }}{66}{figure.caption.83}%
\contentsline {figure}{\numberline {4.14}{\ignorespaces Figura che illustra un esempio di applicazione del rumore generato dal \textbf {MultiscaleNoiseGenerator}.\relax }}{66}{figure.caption.84}%
\contentsline {figure}{\numberline {4.15}{\ignorespaces Nella figura è mostrato il contenuto di un sample che si può ottenere dall'oggetto \textbf {CoiganSeverstalSteelDefectsDataset}, a sinistra i cubi rappresentano i tensori, con i nomi effettivi che sono presenti nel dict, mentre a destra vi sono i canali dei tensori, le immagini raggruppano 3 canali mentre le maschere occupano un solo canale. Si noti che l'ordine da sinistra verso destra è coerente con l'ordine dei canali nei tensori.\relax }}{67}{figure.caption.85}%
\contentsline {figure}{\numberline {4.16}{\ignorespaces Figura che illustra graficamente il processo di calcolo della loss L1 smooth masked.\relax }}{71}{figure.caption.88}%
\contentsline {figure}{\numberline {4.17}{\ignorespaces Figura che illustra concettualmente la struttura interna del discriminatore basato sulla trasformata \textit {wavelet}.\relax }}{74}{figure.caption.90}%
\contentsline {figure}{\numberline {4.18}{\ignorespaces Figura che illustra concettualmente il processo di estrazione dei difetti dall'immagine generata, per l'addestramento del discriminatore.\relax }}{75}{figure.caption.91}%
\contentsline {figure}{\numberline {4.19}{\ignorespaces Figura che illustra un batch di immagini generate, da una precedente versione della training pipeline, non provvista di \textbf {ref discriminator}, e con le loss \textbf {L1} e \textbf {Perceptual loss} che non utilizzano lo smooth della weight mask.\relax }}{76}{figure.caption.92}%
\contentsline {figure}{\numberline {4.20}{\ignorespaces Figura che illustra un batch di immagini generate, da una versione della training pipeline provvista di \textbf {ref discriminator}, e con le loss \textbf {L1} e \textbf {Perceptual loss} che utilizzano lo smooth della weight mask.\relax }}{77}{figure.caption.93}%
\addvspace {10\p@ }
