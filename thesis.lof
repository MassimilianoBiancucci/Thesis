\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Un esempio preso dal \textbf {Severstal steel defect dataset} di difetti segmentati. credits: Neven Robby and Goedemé Toon, 2021, A Multi-Branch U-Net for Steel Surface Defect Type and Severity Segmentation. https://www.mdpi.com/2075-4701/11/6/870\relax }}{2}{figure.caption.3}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces Esempio di architettura a solo decoder.\relax }}{4}{figure.caption.5}%
\contentsline {figure}{\numberline {1.3}{\ignorespaces Esempio di architettura con encoder e decoder.\relax }}{5}{figure.caption.7}%
\contentsline {figure}{\numberline {1.4}{\ignorespaces Un diagramma di ven che illustra le relazioni tra i diversi sottogruppi dell'intelligenza artificiale, vediamo infatti come il deep learning sia un sottogruppo del representation learning, che a sua volta è un sottogruppo del machine learning. credits: Yoshua Bengio, Ian J. Goodfellow, Aaron Courville 2015, From the book "Deep Learning" \relax }}{6}{figure.caption.8}%
\contentsline {figure}{\numberline {1.5}{\ignorespaces Schema di un neurone biologico.\relax }}{7}{figure.caption.9}%
\contentsline {figure}{\numberline {1.6}{\ignorespaces Esempio di rete neurale feedforward. In tale rete è possibile vedere i neuroni rappresentati dai nodi del grafo, e le interconnessioni tra di essi che definiscono il peso di ogni relazione, con in rosso un peso positivo e in blu un peso negativo, l'intensità del colore indica la forza della relazione.\relax }}{8}{figure.caption.10}%
\contentsline {figure}{\numberline {1.7}{\ignorespaces Esempio di Neurone artificiale.\relax }}{8}{figure.caption.11}%
\contentsline {figure}{\numberline {1.9}{\ignorespaces Esempio di retropropagazione su di una rete semplificata a 2 strati.\relax }}{12}{figure.caption.13}%
\contentsline {figure}{\numberline {1.10}{\ignorespaces L'immagine raffigura schematicamente il funzionamento di Neocognitron. credits: Kunihiko Fukushima 1980 \cite {Fukushima1980Neocognitron}.\relax }}{14}{figure.caption.14}%
\contentsline {figure}{\numberline {1.11}{\ignorespaces L'immagine tratta dallo stesso articolo illustra schematicamente l'architettura del modello TDNN. credits: Alex Waibel et al. 1989 \cite {Waibel1989PhonemeRecognition}.\relax }}{14}{figure.caption.15}%
\contentsline {figure}{\numberline {1.12}{\ignorespaces L'immagine tratta dal medesimo articolo illustra l'architettura proposta. credits: Yann LeCun et al. 1989 \cite {LeCun1989Backpropagation}.\relax }}{15}{figure.caption.16}%
\contentsline {figure}{\numberline {1.13}{\ignorespaces L'immagine illustra l'operazione di convoluzione. credits: Wikipedia. https://en.wikipedia.org /wiki/Convolution\relax }}{16}{figure.caption.17}%
\contentsline {figure}{\numberline {1.14}{\ignorespaces Operazione di convoluzione 2D discreta, in blu abbiamo una matrice 5x5 che rappresenta un'immagine, mentre in verde il risultato di una convoluzione, con un kernel 3x3. I valori del kernel sono raffigurati in basso a destra delle caselle interessate dalla convoluzione. credits: Dumoulin et al. 2016 \cite {dumoulin2016guide}.\relax }}{16}{figure.caption.18}%
\contentsline {figure}{\numberline {1.15}{\ignorespaces Immagine di Lena Forsén, prima e dopo la convoluzione con i due kernel, che amplificano i bordi verticali e orizzontali.\relax }}{18}{figure.caption.19}%
\contentsline {figure}{\numberline {1.16}{\ignorespaces Esempio di convoluzione con un kernel 3x3, stride pari a 2, e padding (zero padding) pari a 1. credits: Dumoulin et al. 2016 \cite {dumoulin2016guide}\relax }}{19}{figure.caption.23}%
\contentsline {figure}{\numberline {1.17}{\ignorespaces Esempio di max pooling con una finestra di dimensione 3x3, stride pari a 1 e padding 0. credits: Dumoulin et al. 2016 \cite {dumoulin2016guide}\relax }}{21}{figure.caption.27}%
\contentsline {figure}{\numberline {1.18}{\ignorespaces Esempio di average pooling con una finestra di dimensione 3x3, stride pari a 1 e padding 0. credits: Dumoulin et al. 2016 \cite {dumoulin2016guide}\relax }}{21}{figure.caption.29}%
\contentsline {figure}{\numberline {1.19}{\ignorespaces Esempio di convoluzione di un'immagine rgb. credits: Irhum Shafkat \cite {shafkat2018convolutional}\relax }}{22}{figure.caption.30}%
\contentsline {figure}{\numberline {1.20}{\ignorespaces Esempio di convoluzione multichannel. credits: Irhum Shafkat \cite {shafkat2018convolutional}\relax }}{22}{figure.caption.31}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Rappresentazione grafica della training pipeline di un modello GAN.\relax }}{24}{figure.caption.32}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Confronto tra MSE e Adversarial training. credits: Goodfellow \cite {goodfellow2016ganintro}\relax }}{25}{figure.caption.33}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Visualizzazione schematica della convergenza della distribuzione dei dati generati verso quelli reali, e dell'uscita del discriminatore al variare di $\mathbf {x}$. Si noti che la linea nera tratteggiata rappresenta la distribuzione dei dati reali $p_{data}$, mentre la linea verde continua rappresenta la distribuzione dei dati generati $p_{model}$ e in fine la linea blu tratteggiata rappresenta l'uscita del discriminatore $D(x)$, al variare di $\mathbf {x}$. I 2 assi in fondo sono rispettivamente, l'asse $z$ i valori casuali di input del generatore con distribuzione fissa, e l'asse $x$, i valori di input del discriminatore, appartenenti alla distribuzione $\mathbf {p_{data}}$ o $\mathbf {p_{g}}$. credits: Goodfellow et al. \cite {goodfellow2014generative}\relax }}{27}{figure.caption.35}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces Esempio di corretto apprendimento (prima riga) e di mode collapse (seconda riga). credits: Luke Metz et al. \cite {metz2017unrolled}\relax }}{29}{figure.caption.37}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces Alcuni risultati del modello addestrato in questa ricerca, relativi a un dataset di facce umane in bassa risoluzione (b) e il dataset Minst (a). credits: Goodfellow et al. \cite {goodfellow2014generative}\relax }}{29}{figure.caption.38}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces Architettura del generatore di DCGAN. credits: Alec Radford et al. \cite {radford2016unsupervised}\relax }}{30}{figure.caption.39}%
\contentsline {figure}{\numberline {2.9}{\ignorespaces Esempio di algebra vettoriale nello spazio latente Z. credits: Alec Radford et al. \cite {radford2016unsupervised}\relax }}{31}{figure.caption.40}%
\contentsline {figure}{\numberline {2.10}{\ignorespaces Esempio di \textit {gradient vanishing}, causato da un discriminatore addestrato fino all'ottimo per un generatore non ottimo.\relax }}{32}{figure.caption.41}%
\contentsline {figure}{\numberline {2.11}{\ignorespaces In questa immagine è possibile vedere la derivata della funzione seno, nei punti $\pi /4$, $\pi /2$ e $\pi $, ed è possibile vedere come tale funzione rispetta la \textit {1-Lipschitz continuity}, essendo la sua derivata all'interno dell'intervallo ammesso in ogni punto.\relax }}{34}{figure.caption.42}%
\contentsline {figure}{\numberline {2.12}{\ignorespaces In questa immagine è possibile vedere la derivata della funzione $x^2$, nei punti 0.2, 0.4 e 0.8, in questo caso la funzione non rispetta la \textit {1-Lipschitz continuity}, in quanto la sua derivata cresce rapidamente oltre il limite consentito dopo x=0.4.\relax }}{34}{figure.caption.43}%
\contentsline {figure}{\numberline {2.13}{\ignorespaces In questa immagine è possibile vedere come il critico $C(x)$ sia in grado di mappare efficacemente la distanza tra due distribuzioni $P_{data}$ e $P_{G}$, restituendo dei gradienti utili per il generatore anche quando il critico è addestrato all'ottimo per il dato generatore.\relax }}{34}{figure.caption.44}%
\contentsline {figure}{\numberline {2.14}{\ignorespaces Andamento del training di una DCGAN e di una MLP addestrate utilizzando la \textit {Wasserstein distance}. credits: Martin Arjovsky et al. \cite {arjovsky2017wasserstein}\relax }}{35}{figure.caption.45}%
\contentsline {figure}{\numberline {2.15}{\ignorespaces Andamento del training di una DCGAN e di una MLP addestrate utilizzando la \textit {Jensen-Shannon divergence}. credits: Martin Arjovsky et al. \cite {arjovsky2017wasserstein}\relax }}{35}{figure.caption.46}%
\contentsline {figure}{\numberline {2.16}{\ignorespaces In questa immagine è possibile vedere un esempio di training di Pix2Pix per il task \textit {edges to photo}, In tale configurazione il discriminatore apprende come discernere tra coppie di immagini e schizzi, e si può vedere come diversamente dal caso classico sia il generatore che il discriminatore ricevono in ingresso l'immagine di riferimento (lo schizzo) credits: Phillip Isola et al. \cite {isola2018imagetoimage}\relax }}{37}{figure.caption.47}%
\contentsline {figure}{\numberline {2.17}{\ignorespaces In questa immagine è illustrata intuitivamente la differenza tra un comune modello ad \textit {encoder-decoder} e l'architettura di U-net. credits: Phillip Isola et al. \cite {isola2018imagetoimage}\relax }}{37}{figure.caption.48}%
\contentsline {figure}{\numberline {2.18}{\ignorespaces In questa immagine è possibile vedere una comparazione di un sample generato da 4 diversi modelli per il task \textit {labels to scene}, i quattro esempi illustrano l'utilizzo o meno delle \textit {skip connection} e l'utilizzo di una loss L1 a una loss L1 più \textit {conditional adversarial loss}. credits: Phillip Isola et al. \cite {isola2018imagetoimage}\relax }}{38}{figure.caption.49}%
\contentsline {figure}{\numberline {2.19}{\ignorespaces In questa immagine sono mostrati alcuni esempi di applicazione di pix2pix. credits: Phillip Isola et al. \cite {isola2018imagetoimage}\relax }}{38}{figure.caption.50}%
\contentsline {figure}{\numberline {2.20}{\ignorespaces In questa immagine è illustrata schematicamente la struttura di LaMa. credits: Roman Suvorov et al. \cite {suvorov2021resolutionrobust}\relax }}{39}{figure.caption.51}%
\contentsline {figure}{\numberline {2.21}{\ignorespaces In questa immagine sono mostrati schematicamente i passaggi che permettono di calcolare la perceptual loss da un modello convoluzionale. credits: Richard Zhang et al. \cite {zhang2018unreasonable}\relax }}{42}{figure.caption.55}%
\contentsline {figure}{\numberline {2.22}{\ignorespaces In questa immagine sono mostrati alcuni risultati ottenuti da LaMa. In blu sono rappresentate le maschere applicate alle immagini, e dunque corrispondenti alle arre che sono state rimosse prima della propagazione nella rete per la ricostruzione. Le immagini sulla destra rappresentano l'output del modello. credits: Roman Suvorov et al. \cite {suvorov2021resolutionrobust}\relax }}{43}{figure.caption.59}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
